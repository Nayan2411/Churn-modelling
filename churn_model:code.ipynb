{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5920c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from IPython.display import display\n",
    "from time import strftime, gmtime\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, KBinsDiscretizer, LabelEncoder\n",
    "# Column Transformer\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602a5bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('downloads/Master_churn_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e397a7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5d471",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11454779",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d9d017",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fefe77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency tables for each categorical feature\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    display(pd.crosstab(index=df[column], columns='% observations', normalize='columns'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0c090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms for each numeric features\n",
    "hist = df.hist(bins=30, sharey=True, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdb5a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Churn'] = df['Churn'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3865979",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['advertiser_id'] = df['advertiser_id'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41357c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    if column != 'Churn':\n",
    "        display(pd.crosstab(index=df[column], columns=df['Churn'], normalize='columns'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b93cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.select_dtypes(exclude=['object']).columns:\n",
    "    print(column)\n",
    "    hist = df[[column, 'Churn']].hist(by='Churn', bins=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9700cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a119fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix = pd.plotting.scatter_matrix(df, figsize=(12, 12))\n",
    "\n",
    "for ax in scatter_matrix.ravel():\n",
    "    ax.set_xlabel(ax.get_xlabel(), fontsize = 10, rotation = 45)\n",
    "    ax.set_ylabel(ax.get_ylabel(), fontsize = 10, rotation = 45)        \n",
    "    \n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a134a8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are highly correlated with other columns\n",
    "df.drop(columns=['rext_USD_plan_rate'], inplace=True)\n",
    "df.drop(columns=['advertiser_id'], inplace=True)\n",
    "df.drop(columns=['advertiser_name'], inplace=True)\n",
    "df.drop(columns=['global_account_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1a3cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing data\n",
    "total = df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea23ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eb4562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode_features =  [\"Churn?\", \"State\", \"Int'l Plan\", \"VMail Plan\"]\n",
    "# df['Churn?'].unique()\n",
    "\n",
    "sales_geo_team_list = ['UK', 'EE', 'BENELUX', 'FRANCE', 'NORDICS', 'DACH', 'IBERIA',\n",
    "       'TURKEY', 'ITALY', 'EMEA', 'MEA', 'EASTERN EUROPE', 'GROUP', 'CMR',\n",
    "       'NORTHERN EUROPE', 'AMS BRAND AS ATO', 'CORE US',\n",
    "       'SOUTH EAST ASIA', 'RETAIL BNL', 'RUSSIA', 'NB CORE US',\n",
    "       'SUPPLY - CPP - GERMANY', 'BRAZIL', 'BRAND FRANCE AS', 'NB LATAM']\n",
    "sales_subregion_list = ['CORE EUROPE', 'EMERGING MARKETS', 'AGENCY', 'NEW BUSINESS',\n",
    "       'KSA EUROPE', 'EUROPE', 'EMEA', 'GROUP', 'AMERICAS BRAND AS', 'US',\n",
    "       'SOUTH ASIA', 'EMEA RETAIL NEE', 'SUPPLY - CPP - EMEA', 'LATAM',\n",
    "       'EMEA BRAND SEE', 'EMEA RETAIL SEE']\n",
    "sales_channel_list = ['Direct', 'Agency']\n",
    "product_taxonomy_label_list = ['commerce growth - acquisition',\n",
    "       'commerce growth - retention retargeting',\n",
    "       'commerce growth - retention', 'unknown', 'undefined']\n",
    "Stream_list = ['CORE', 'EM', 'KSA', 'AG', 'EMEA', 'NEW BUSINESS']\n",
    "Churn_list = ['True.','False.']\n",
    "\n",
    "# Encode Class Labels to integers\n",
    "sales_geo_team_le = LabelEncoder()\n",
    "sales_geo_team_le.fit(sales_geo_team_list)\n",
    "\n",
    "sales_subregion_le = LabelEncoder()\n",
    "sales_subregion_le.fit(sales_subregion_list)\n",
    "\n",
    "sales_channel_le = LabelEncoder()\n",
    "sales_channel_le.fit(sales_channel_list)\n",
    "\n",
    "product_taxonomy_label_le = LabelEncoder()\n",
    "product_taxonomy_label_le.fit(product_taxonomy_label_list)\n",
    "\n",
    "stream_le = LabelEncoder()\n",
    "stream_le.fit(Stream_list)\n",
    "\n",
    "churn_le = LabelEncoder()\n",
    "churn_le.fit(Churn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d711a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a14ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode specific columns\n",
    "df['sales_geo_team'] = sales_geo_team_le.transform(df['sales_geo_team'])\n",
    "df['sales_subregion']= sales_subregion_le.transform(df['sales_subregion'])\n",
    "df['sales_channel'] = sales_channel_le.transform(df['sales_channel'])\n",
    "df['product_taxonomy_label'] = product_taxonomy_label_le.transform(df['product_taxonomy_label'])\n",
    "df['Stream'] = stream_le.transform(df['Stream'])\n",
    "df['Churn'] = churn_le.transform(df['Churn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cb15ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e759e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Churn'] = df['Churn'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951b98b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features =['sales_geo_team','sales_subregion','sales_channel','product_taxonomy_label', 'Stream']\n",
    "\n",
    "numeric_features = ['displays','clicks',\n",
    "                    'revenue_USD_plan_rate','no_of_days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9256a75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features + numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43381658",
   "metadata": {},
   "outputs": [],
   "source": [
    "colTransformer = ColumnTransformer([('onehot',\n",
    "                                     OneHotEncoder(categories='auto',sparse=False),\n",
    "                                     categorical_features),\n",
    "                                    ('standardize',\n",
    "                                    StandardScaler(),numeric_features)\n",
    "                                   ],\n",
    "                                   remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e605b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "colTransformer.fit(df[categorical_features + numeric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29743595",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = np.split(df.sample(frac=1, random_state=1729), \n",
    "                                                  [int(0.7 * len(df)), int(0.9 * len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30e789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape,validation_data.shape,test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb393974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output columns\n",
    "columns = [ \"Churn\",\n",
    "\"sales_geo_team\",\n",
    "\"sales_subregion\",\n",
    "\"sales_channel\",\n",
    "\"product_taxonomy_label\" ,\"Stream\"        ,\"displays\",\"clicks\",\"revenue_USD_plan_rate\",\"no_of_days\"  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35416f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_transformed = colTransformer.transform (train_data[categorical_features + numeric_features])\n",
    "\n",
    "validation_data_transformed = colTransformer.transform (validation_data[categorical_features + numeric_features])\n",
    "\n",
    "test_data_transformed = colTransformer.transform (test_data[categorical_features + numeric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85ac12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_transformed.shape, validation_data_transformed.shape, test_data_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567a8f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Churn'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6a2cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_transformed = np.concatenate((np.array([train_data['Churn']]).T, train_data_transformed),axis=1)\n",
    "validation_data_transformed = np.concatenate((np.array([validation_data['Churn']]).T, validation_data_transformed),axis=1)\n",
    "test_data_transformed = np.concatenate((np.array([test_data['Churn']]).T, test_data_transformed),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e22349",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_transformed.shape, validation_data_transformed.shape, test_data_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32778d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_transformed[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17a4218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/6081008/dump-a-numpy-array-into-a-csv-file\n",
    "# Write Training Set\n",
    "np.savetxt('train_onehot.csv',train_data_transformed,delimiter=\",\",fmt='%.5e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ef1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Validation Set\n",
    "np.savetxt('validation_onehot.csv',validation_data_transformed,delimiter=\",\",fmt='%.5e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a0390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Test Set\n",
    "np.savetxt('test_onehot.csv',test_data_transformed,delimiter=\",\",fmt='%.5e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b25057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Training Set\n",
    "train_data.to_csv('train.csv'\n",
    "                          ,index=False,header=False\n",
    "                          ,columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bd9883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Validation Set\n",
    "validation_data.to_csv('validation.csv'\n",
    "                          ,index=False,header=False\n",
    "                          ,columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a792ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Test Set\n",
    "test_data.to_csv('test.csv'\n",
    "                          ,index=False,header=False\n",
    "                          ,columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45094590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Column List\n",
    "with open('column_list.txt','w') as f:\n",
    "    f.write(','.join(columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ac0fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow==1.2.0 --ignore-installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b82d01",
   "metadata": {},
   "outputs": [],
   "source": [
    " #pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebc5ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/\n",
    "# https://github.com/keras-team/keras/issues/2743\n",
    "# Change Kernel to use Tensor Flow. For example: conda_tensorflow_p36\n",
    "import sys\n",
    "import numpy as np\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Column Transformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, KBinsDiscretizer\n",
    "\n",
    "# Keras Library\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f9f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f4448c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529009a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'train_onehot.csv'\n",
    "validation_file = 'validation_onehot.csv'\n",
    "test_file = 'test_onehot.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06425218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the column names as the file does not have column header\n",
    "df_train = pd.read_csv(train_file, header=None)\n",
    "df_validation = pd.read_csv(validation_file, header=None)\n",
    "df_test = pd.read_csv(test_file, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b369cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fca9806",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.iloc[:,1:] # Features: 1st column onwards \n",
    "y_train = df_train.iloc[:,0].ravel() # Target: 0th column\n",
    "\n",
    "X_validation = df_validation.iloc[:,1:] # Features: 1st column onwards \n",
    "y_validation = df_validation.iloc[:,0].ravel() # Target: 0th column\n",
    "\n",
    "X_test = df_test.iloc[:,1:] # Features: 1st column onwards \n",
    "y_test = df_test.iloc[:,0].ravel() # Target: 0th column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8d6d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/getting-started/sequential-model-guide/\n",
    "model = Sequential()\n",
    "# 1 hidden layer with 30 neurons with relu activation\n",
    "# output layer - binaryclassification, so use sigmoid activation\n",
    "# optimizer - use adam or rmsprop\n",
    "# loss function - logistic loss function - called as binary cross entropy in keras\n",
    "# metrics - additional metrics to report\n",
    "model.add(Dense(30, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66a98b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d46dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7c8c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32,\n",
    "         validation_data=(X_validation,y_validation),callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90ce09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=history.epoch,y=history.history['loss'],label='Training Error')\n",
    "plt.scatter(x=history.epoch,y=history.history['val_loss'],label='Validation Error')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Vs Validation Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9902a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts a binary outcome for each observation\n",
    "result = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d982652",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff332998",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('results.csv',result,delimiter=\",\",fmt='%.5e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e233338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list_file = 'column_list.txt'\n",
    "test_file = 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab5c5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ''\n",
    "with open(column_list_file,'r') as f:\n",
    "    columns = f.read().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f154b011",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(test_file,names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c63ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['predicted_prob'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b525ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['predicted_class'] = np.where(result > 0.5,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cff721",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['Churn','predicted_class', 'predicted_prob']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b858a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "# Explicitly stating labels. Pass=1, Fail=0\n",
    "def true_positive(y_true, y_pred): \n",
    "    return confusion_matrix(y_true, y_pred,labels=[1,0])[0, 0]\n",
    "\n",
    "def true_negative(y_true, y_pred): \n",
    "    return confusion_matrix(y_true,y_pred,labels=[1,0])[1, 1]\n",
    "\n",
    "def false_positive(y_true, y_pred): \n",
    "    return confusion_matrix(y_true, y_pred,labels=[1,0])[1, 0]\n",
    "\n",
    "def false_negative(y_true, y_pred): \n",
    "    return confusion_matrix(y_true, y_pred,labels=[1,0])[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d625ef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Binary Classifier Metrics\n",
    "# Returns a dictionary {\"MetricName\":Value,...}\n",
    "\n",
    "def binary_classifier_metrics(y_true, y_pred):\n",
    "    metrics = {}\n",
    "\n",
    "    # References: \n",
    "    #  https://docs.aws.amazon.com/machine-learning/latest/dg/binary-classification.html\n",
    "    #  https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "    \n",
    "    # Definition:\n",
    "    # true positive = tp = how many samples were correctly classified as positive (count)\n",
    "    # true negative = tn = how many samples were correctly classified as negative (count)\n",
    "    # false positive = fp = how many negative samples were mis-classified as positive (count)\n",
    "    # false_negative = fn = how many positive samples were mis-classified as negative (count)\n",
    "    \n",
    "    # positive = number of positive samples (count)\n",
    "    #          = true positive + false negative\n",
    "    # negative = number of negative samples (count)\n",
    "    #          = true negative + false positive\n",
    "    \n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    tn = true_negative(y_true, y_pred)\n",
    "    fp = false_positive(y_true, y_pred)\n",
    "    fn = false_negative(y_true, y_pred)\n",
    "    \n",
    "    positive = tp + fn\n",
    "    negative = tn + fp\n",
    "    \n",
    "    metrics['TruePositive'] = tp\n",
    "    metrics['TrueNegative'] = tn\n",
    "    metrics['FalsePositive'] = fp\n",
    "    metrics['FalseNegative'] = fn\n",
    "    \n",
    "    metrics['Positive'] = positive\n",
    "    metrics['Negative'] = negative\n",
    "    \n",
    "    # True Positive Rate (TPR, Recall) = true positive/positive\n",
    "    # How many positives were correctly classified? (fraction)\n",
    "    # Recall value closer to 1 is better. closer to 0 is worse\n",
    "    if tp == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = tp/positive\n",
    "        \n",
    "    metrics['Recall'] = recall\n",
    "    \n",
    "    # True Negative Rate = True Negative/negative\n",
    "    # How many negatives were correctly classified? (fraction)\n",
    "    # True Negative Rate value closer to 1 is better. closer to 0 is worse\n",
    "    if tn == 0:\n",
    "        tnr = 0\n",
    "    else:\n",
    "        tnr = tn/(negative)\n",
    "    metrics['TrueNegativeRate'] = tnr\n",
    "    \n",
    "    # Precision = True Positive/(True Positive + False Positive)\n",
    "    # How many positives classified by the algorithm are really positives? (fraction)\n",
    "    # Precision value closer to 1 is better. closer to 0 is worse\n",
    "    if tp == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = tp/(tp + fp)\n",
    "    metrics['Precision'] = precision\n",
    "    \n",
    "    # Accuracy = (True Positive + True Negative)/(total positive + total negative)\n",
    "    # How many positives and negatives were correctly classified? (fraction)\n",
    "    # Accuracy value closer to 1 is better. closer to 0 is worse\n",
    "    accuracy = (tp + tn)/(positive + negative)\n",
    "    metrics['Accuracy'] = accuracy\n",
    "    \n",
    "    # False Positive Rate (FPR, False Alarm) = False Positive/(total negative)\n",
    "    # How many negatives were mis-classified as positives (fraction)\n",
    "    # False Positive Rate value closer to 0 is better. closer to 1 is worse\n",
    "    if fp == 0:\n",
    "        fpr = 0\n",
    "    else:\n",
    "        fpr = fp/(negative)\n",
    "    metrics['FalsePositiveRate'] = fpr\n",
    "    \n",
    "    # False Negative Rate (FNR, Misses) = False Negative/(total Positive)\n",
    "    # How many positives were mis-classified as negative (fraction)\n",
    "    # False Negative Rate value closer to 0 is better. closer to 1 is worse\n",
    "    fnr = fn/(positive)\n",
    "    metrics['FalseNegativeRate'] = fnr\n",
    "    \n",
    "    # F1 Score = harmonic mean of Precision and Recall\n",
    "    # F1 Score closer to 1 is better. Closer to 0 is worse.\n",
    "    if precision == 0 or recall == 0:\n",
    "        f1 = 0\n",
    "    else:        \n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "    metrics['F1'] = f1\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e64db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: \n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "    #    print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c60525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "# 1 = customer left/churn, 0 = stayed\n",
    "cnf_matrix = confusion_matrix(df_test['Churn'], df_test['predicted_class'],labels=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a708d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix,classes=['Churn','Stay'],\n",
    "                      title='Customer Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1db891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Churn','Stay'],\n",
    "                      title='Customer Churn - Fraction', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b615f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [binary_classifier_metrics(df_test['Churn'], df_test['predicted_class'])]\n",
    "df_metrics=pd.DataFrame.from_dict(metrics)\n",
    "df_metrics.index = ['Model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c39cdab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2aa074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
